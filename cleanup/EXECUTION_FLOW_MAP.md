# Execution Flow Map

**Created**: 2025-12-27  
**Purpose**: Complete call tree mapping of the main execution flow from entry point to database writes  
**Status**: Phase 1, Step 1.1 - In Progress

---

## üìã Overview

This document traces every function call in the main execution path from `run_cycles.sh` through the complete data pipeline to database writes. This is used to identify which code is actually used vs unused.

---

## üîÑ Complete Execution Flow

### Entry Point: `run_cycles.sh`

**File**: `run_cycles.sh` (root directory)  
**Lines**: 72-73

```bash
curl -s -X POST "http://localhost:8000/agent/test-cycle-no-auth?test_user_id=$USER_ID"
```

**What it does**:
- Makes HTTP POST request to backend API
- Passes `user_id` as query parameter
- Monitors `logs/automatic_scheduling.log` for completion

---

### API Endpoint: `/agent/test-cycle-no-auth`

**File**: `src/api/service.py`  
**Lines**: ~1334-1385 (needs verification)

**Function Signature**:
```python
@app.post("/agent/test-cycle-no-auth")
async def test_cycle_no_auth(
    test_user_id: str = Query(...),
    background_tasks: BackgroundTasks = BackgroundTasks()
)
```

**Call Chain**:
1. Extracts `test_user_id` from query params
2. Calls `agent.run_single_cycle_parallel(test_user_id)`
3. Returns immediately (async execution)
4. Logs to `logs/automatic_scheduling.log`

---

### Core Agent: `run_single_cycle_parallel()`

**File**: `src/agent/core.py`  
**Lines**: 1962-2073  
**Class**: `SentimentAnalysisAgent`  
**Method**: `run_single_cycle_parallel(user_id: str)`

**Complete Call Tree**:

```
run_single_cycle_parallel(user_id)
‚îÇ
‚îú‚îÄ> Phase 1: Data Collection
‚îÇ   ‚îî‚îÄ> _run_task(lambda: self.collect_data_parallel(user_id), 'collect_user_{user_id}')
‚îÇ       ‚îÇ
‚îÇ       ‚îî‚îÄ> collect_data_parallel(user_id) [line 829]
‚îÇ           ‚îÇ
‚îÇ           ‚îú‚îÄ> db_factory() ‚Üí creates DB session
‚îÇ           ‚îÇ
‚îÇ           ‚îú‚îÄ> _get_latest_target_config(db, user_id) [line ~789]
‚îÇ           ‚îÇ   ‚îî‚îÄ> Query TargetIndividualConfiguration table
‚îÇ           ‚îÇ
‚îÇ           ‚îú‚îÄ> _get_enabled_collectors_for_target(target_name) [line 908]
‚îÇ           ‚îÇ   ‚îÇ
‚îÇ           ‚îÇ   ‚îú‚îÄ> TargetConfigManager() [from src.collectors.target_config_manager]
‚îÇ           ‚îÇ   ‚îÇ   ‚îî‚îÄ> _load_config() [loads config/target_configs.json]
‚îÇ           ‚îÇ   ‚îÇ
‚îÇ           ‚îÇ   ‚îú‚îÄ> config_manager.get_target_by_name(target_name)
‚îÇ           ‚îÇ   ‚îÇ
‚îÇ           ‚îÇ   ‚îî‚îÄ> config_manager.get_enabled_collectors(target_id)
‚îÇ           ‚îÇ
‚îÇ           ‚îî‚îÄ> _run_collectors_parallel(enabled_collectors, queries_json, user_id) [line 942]
‚îÇ               ‚îÇ
‚îÇ               ‚îú‚îÄ> get_collection_tracker() [from src.utils.collection_tracker]
‚îÇ               ‚îÇ   ‚îî‚îÄ> CollectionTracker instance
‚îÇ               ‚îÇ
‚îÇ               ‚îú‚îÄ> For each collector:
‚îÇ               ‚îÇ   ‚îú‚îÄ> tracker.get_incremental_date_range(user_id, source_type)
‚îÇ               ‚îÇ   ‚îÇ   ‚îî‚îÄ> Query database for last collection date
‚îÇ               ‚îÇ   ‚îÇ
‚îÇ               ‚îÇ   ‚îî‚îÄ> subprocess.run([
‚îÇ               ‚îÇ       "python", "-m", f"src.collectors.{collector_name}",
‚îÇ               ‚îÇ       queries_json, user_id
‚îÇ               ‚îÇ   ])
‚îÇ               ‚îÇ   ‚îî‚îÄ> Collector executes (separate process)
‚îÇ               ‚îÇ       ‚îî‚îÄ> Writes CSV to data/raw/{collector_name}_{timestamp}.csv
‚îÇ               ‚îÇ
‚îÇ               ‚îî‚îÄ> ThreadPoolExecutor manages parallel execution
‚îÇ
‚îú‚îÄ> Phase 2: Load Raw Data
‚îÇ   ‚îî‚îÄ> _run_task(lambda: self._push_raw_data_to_db(user_id), 'load_raw_{user_id}')
‚îÇ       ‚îÇ
‚îÇ       ‚îî‚îÄ> _push_raw_data_to_db(user_id) [line ~2300-2422]
‚îÇ           ‚îÇ
‚îÇ           ‚îú‚îÄ> glob.glob(str(self.base_path / 'data' / 'raw' / '*.csv'))
‚îÇ           ‚îÇ   ‚îî‚îÄ> Finds all CSV files in data/raw/
‚îÇ           ‚îÇ
‚îÇ           ‚îú‚îÄ> For each CSV file:
‚îÇ           ‚îÇ   ‚îú‚îÄ> pd.read_csv(file_path)
‚îÇ           ‚îÇ   ‚îî‚îÄ> Convert DataFrame rows to dictionaries
‚îÇ           ‚îÇ
‚îÇ           ‚îî‚îÄ> Store in self._temp_raw_records (list of dicts)
‚îÇ
‚îú‚îÄ> Phase 3: Deduplication
‚îÇ   ‚îî‚îÄ> _run_task(lambda: self._run_deduplication(user_id), 'dedup_{user_id}')
‚îÇ       ‚îÇ
‚îÇ       ‚îî‚îÄ> _run_deduplication(user_id) [line 2423]
‚îÇ           ‚îÇ
‚îÇ           ‚îú‚îÄ> db_factory() ‚Üí creates DB session
‚îÇ           ‚îÇ
‚îÇ           ‚îú‚îÄ> self.deduplication_service.deduplicate_new_data(
‚îÇ           ‚îÇ       self._temp_raw_records, db, user_id
‚îÇ           ‚îÇ   ) [src/utils/deduplication_service.py]
‚îÇ           ‚îÇ   ‚îÇ
‚îÇ           ‚îÇ   ‚îú‚îÄ> find_existing_duplicates(new_records, db, user_id)
‚îÇ           ‚îÇ   ‚îÇ   ‚îú‚îÄ> Query SentimentData table for existing records
‚îÇ           ‚îÇ   ‚îÇ   ‚îú‚îÄ> normalize_text() for each record
‚îÇ           ‚îÇ   ‚îÇ   ‚îî‚îÄ> is_similar_text() comparison
‚îÇ           ‚îÇ   ‚îÇ
‚îÇ           ‚îÇ   ‚îî‚îÄ> Returns: {
‚îÇ           ‚îÇ       'unique_records': [...],
‚îÇ           ‚îÇ       'duplicates': [...],
‚îÇ           ‚îÇ       'stats': {...}
‚îÇ           ‚îÇ   }
‚îÇ           ‚îÇ
‚îÇ           ‚îú‚îÄ> deduplication_service.get_deduplication_summary(results)
‚îÇ           ‚îÇ
‚îÇ           ‚îî‚îÄ> Bulk insert unique records to database:
‚îÇ               ‚îú‚îÄ> For each record in unique_records:
‚îÇ               ‚îÇ   ‚îú‚îÄ> _parse_date_string() - Parse date fields
‚îÇ               ‚îÇ   ‚îú‚îÄ> _validate_and_clean_location() - Clean location data
‚îÇ               ‚îÇ   ‚îî‚îÄ> Create SentimentData object
‚îÇ               ‚îÇ
‚îÇ               ‚îî‚îÄ> db.bulk_insert_mappings(SentimentData, bulk_data)
‚îÇ               ‚îî‚îÄ> db.commit()
‚îÇ
‚îú‚îÄ> Phase 4: Sentiment & Governance Analysis
‚îÇ   ‚îî‚îÄ> _run_task(lambda: self._run_sentiment_batch_update_parallel(user_id), 'sentiment_batch_{user_id}')
‚îÇ       ‚îÇ
‚îÇ       ‚îî‚îÄ> _run_sentiment_batch_update_parallel(user_id) [line ~2575-2650]
‚îÇ           ‚îÇ
‚îÇ           ‚îú‚îÄ> db_factory() ‚Üí creates DB session
‚îÇ           ‚îÇ
‚îÇ           ‚îú‚îÄ> Query SentimentData:
‚îÇ           ‚îÇ   WHERE sentiment_label IS NULL
‚îÇ           ‚îÇ   AND user_id = user_id
‚îÇ           ‚îÇ   LIMIT 10000
‚îÇ           ‚îÇ
‚îÇ           ‚îú‚îÄ> Create batches (size = self.sentiment_batch_size, default 150)
‚îÇ           ‚îÇ
‚îÇ           ‚îî‚îÄ> _process_sentiment_batches_parallel(batches, user_id) [line 2652]
‚îÇ               ‚îÇ
‚îÇ               ‚îî‚îÄ> ThreadPoolExecutor processes batches in parallel
‚îÇ                   ‚îÇ
‚îÇ                   ‚îî‚îÄ> For each batch (process_single_batch):
‚îÇ                       ‚îÇ
‚îÇ                       ‚îú‚îÄ> db_factory() ‚Üí creates DB session for thread
‚îÇ                       ‚îÇ
‚îÇ                       ‚îú‚îÄ> For each record:
‚îÇ                       ‚îÇ   ‚îî‚îÄ> db.merge(record) - Merge into thread's session
‚îÇ                       ‚îÇ
‚îÇ                       ‚îú‚îÄ> Extract texts from records
‚îÇ                       ‚îÇ
‚îÇ                       ‚îú‚îÄ> self.data_processor.batch_get_sentiment(
‚îÇ                       ‚îÇ       texts_list, source_types_list, max_workers
‚îÇ                       ‚îÇ   ) [src/processing/data_processor.py]
‚îÇ                       ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ   ‚îú‚îÄ> RecordRouter.route_records(texts, source_types)
‚îÇ                       ‚îÇ   ‚îÇ   ‚îî‚îÄ> Distributes records across model pipelines
‚îÇ                       ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ   ‚îú‚îÄ> For each model pipeline (parallel):
‚îÇ                       ‚îÇ   ‚îÇ   ‚îú‚îÄ> presidential_sentiment_analyzer.analyze(text)
‚îÇ                       ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ> [src/processing/presidential_sentiment_analyzer.py]
‚îÇ                       ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ> OpenAI API call for sentiment
‚îÇ                       ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ> Returns: sentiment_label, sentiment_score, 
‚îÇ                       ‚îÇ   ‚îÇ   ‚îÇ                    sentiment_justification, embedding
‚îÇ                       ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ   ‚îÇ   ‚îî‚îÄ> governance_analyzer.analyze(text, source_type, sentiment)
‚îÇ                       ‚îÇ   ‚îÇ       ‚îî‚îÄ> [src/processing/governance_analyzer.py]
‚îÇ                       ‚îÇ   ‚îÇ           ‚îú‚îÄ> OpenAI API call for ministry classification
‚îÇ                       ‚îÇ   ‚îÇ           ‚îú‚îÄ> OpenAI API call for issue classification
‚îÇ                       ‚îÇ   ‚îÇ           ‚îî‚îÄ> Returns: ministry_hint, issue_slug, 
‚îÇ                       ‚îÇ   ‚îÇ                        issue_label, issue_confidence, 
‚îÇ                       ‚îÇ   ‚îÇ                        issue_keywords
‚îÇ                       ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ   ‚îî‚îÄ> Combine results from all pipelines
‚îÇ                       ‚îÇ
‚îÇ                       ‚îú‚îÄ> For each record + analysis_result:
‚îÇ                       ‚îÇ   ‚îú‚îÄ> Update record fields:
‚îÇ                       ‚îÇ   ‚îÇ   ‚îú‚îÄ> sentiment_label
‚îÇ                       ‚îÇ   ‚îÇ   ‚îú‚îÄ> sentiment_score
‚îÇ                       ‚îÇ   ‚îÇ   ‚îú‚îÄ> sentiment_justification
‚îÇ                       ‚îÇ   ‚îÇ   ‚îú‚îÄ> ministry_hint
‚îÇ                       ‚îÇ   ‚îÇ   ‚îú‚îÄ> issue_slug
‚îÇ                       ‚îÇ   ‚îÇ   ‚îú‚îÄ> issue_label
‚îÇ                       ‚îÇ   ‚îÇ   ‚îú‚îÄ> issue_confidence
‚îÇ                       ‚îÇ   ‚îÇ   ‚îî‚îÄ> issue_keywords (JSON)
‚îÇ                       ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ   ‚îî‚îÄ> Store embedding:
‚îÇ                       ‚îÇ       ‚îú‚îÄ> Check SentimentEmbedding table for existing
‚îÇ                       ‚îÇ       ‚îú‚îÄ> Create/Update SentimentEmbedding record
‚îÇ                       ‚îÇ       ‚îî‚îÄ> embedding = json.dumps(embedding_data)
‚îÇ                       ‚îÇ
‚îÇ                       ‚îú‚îÄ> db.commit() - Commit all updates
‚îÇ                       ‚îÇ
‚îÇ                       ‚îî‚îÄ> Return processed count
‚îÇ
‚îî‚îÄ> Phase 5: Location Classification
    ‚îî‚îÄ> _run_task(lambda: self._run_location_batch_update_parallel(user_id), 'location_batch_{user_id}')
        ‚îÇ
        ‚îî‚îÄ> _run_location_batch_update_parallel(user_id) [line ~2780-2850]
            ‚îÇ
            ‚îú‚îÄ> db_factory() ‚Üí creates DB session
            ‚îÇ
            ‚îú‚îÄ> Query SentimentData:
            ‚îÇ   WHERE location_label IS NULL
            ‚îÇ   AND user_id = user_id
            ‚îÇ   LIMIT 10000
            ‚îÇ
            ‚îú‚îÄ> Create batches (size = self.location_batch_size, default 300)
            ‚îÇ
            ‚îî‚îÄ> _process_location_batches_parallel(batches, user_id) [line ~2923]
                ‚îÇ
                ‚îî‚îÄ> ThreadPoolExecutor processes batches in parallel
                    ‚îÇ
                    ‚îî‚îÄ> For each batch:
                        ‚îÇ
                        ‚îú‚îÄ> db_factory() ‚Üí creates DB session for thread
                        ‚îÇ
                        ‚îú‚îÄ> For each record:
                        ‚îÇ   ‚îú‚îÄ> Extract text content
                        ‚îÇ   ‚îú‚îÄ> Simple location classifier (keyword matching)
                        ‚îÇ   ‚îÇ   ‚îî‚îÄ> _classify_location() [defined in core.py]
                        ‚îÇ   ‚îÇ       ‚îî‚îÄ> Keyword matching against location keywords
                        ‚îÇ   ‚îÇ
                        ‚îÇ   ‚îî‚îÄ> Update record:
                        ‚îÇ       ‚îú‚îÄ> location_label
                        ‚îÇ       ‚îî‚îÄ> location_confidence
                        ‚îÇ
                        ‚îú‚îÄ> db.commit()
                        ‚îÇ
                        ‚îî‚îÄ> Return processed count
```

---

## üîç Helper Functions Called

### Task Management

**`_run_task(task_func: Callable, task_name: str)`** [line 1880]
- Handles task execution, locking, error handling
- Calls `_check_and_release_stuck_lock()` if needed
- Updates `self.task_status`
- Logs task execution

**`_check_and_release_stuck_lock(task_name: str)`** [line ~1713]
- Checks if lock is stuck (exceeded max age)
- Releases lock if stuck
- Logs stuck lock release

### Database & Configuration

**`_get_latest_target_config(db: Session, user_id: str)`** [line ~789]
- Queries `TargetIndividualConfiguration` table
- Returns latest config for user

**`_parse_date_string(date_str)`** [used in deduplication]
- Parses various date formats
- Returns datetime or None

**`_validate_and_clean_location(location)`** [line ~680]
- Validates and cleans location strings
- Returns cleaned location or None

### Location Classification

**`_classify_location(text: str)`** [defined in core.py, ~line 2141]
- Simple keyword-based location classifier
- Returns location_label and confidence

---

## üì¶ External Dependencies

### Modules Imported and Used

1. **`src.collectors.target_config_manager`**
   - `TargetConfigManager` class
   - Used in `_get_enabled_collectors_for_target()`

2. **`src.utils.collection_tracker`**
   - `get_collection_tracker()` function
   - `CollectionTracker` class
   - Used in `_run_collectors_parallel()`

3. **`src.utils.deduplication_service`**
   - `DeduplicationService` class
   - Used in `_run_deduplication()`

4. **`src.processing.data_processor`**
   - `DataProcessor` class
   - Used in `_process_sentiment_batches_parallel()`

5. **`src.processing.presidential_sentiment_analyzer`**
   - `PresidentialSentimentAnalyzer` class
   - Used by `DataProcessor.batch_get_sentiment()`

6. **`src.processing.governance_analyzer`**
   - `GovernanceAnalyzer` class
   - Used by `DataProcessor.batch_get_sentiment()`

7. **`src.processing.record_router`**
   - `RecordRouter` class (likely)
   - Used by `DataProcessor.batch_get_sentiment()`

8. **`src.api.models`**
   - `SentimentData` model
   - `SentimentEmbedding` model
   - `TargetIndividualConfiguration` model
   - Used throughout for database operations

9. **`src.api.database`**
   - `SessionLocal` (db_factory)
   - Used for all database operations

---

## üóÑÔ∏è Database Operations

### Tables Accessed

1. **`target_individual_configurations`**
   - READ: `_get_latest_target_config()`

2. **`sentiment_data`**
   - READ: Deduplication queries, sentiment/location batch queries
   - WRITE: Bulk insert in deduplication, UPDATE in sentiment/location analysis

3. **`sentiment_embeddings`**
   - READ: Check for existing embeddings
   - WRITE: Insert/update embeddings

4. **`collection_tracker`** (if table exists)
   - READ: Get last collection dates
   - WRITE: Update collection dates

---

## üìÅ File System Operations

### Files Read

1. **`config/target_configs.json`**
   - Read by `TargetConfigManager._load_config()`

2. **`data/raw/*.csv`**
   - Read by `_push_raw_data_to_db()`
   - Glob pattern: `data/raw/*.csv`

### Files Written

1. **`data/raw/{collector_name}_{timestamp}.csv`**
   - Written by individual collectors (separate processes)

2. **`logs/automatic_scheduling.log`**
   - Written by auto_schedule_logger throughout execution

3. **`logs/collectors/{collector_name}/{collector_name}_{timestamp}.log`**
   - Written by collectors (separate processes)

4. **`logs/agent.log`**
   - Written by logger throughout execution

---

## üîÑ Subprocess Calls

### Collector Execution

Each collector runs as a separate Python process:

```python
subprocess.run([
    sys.executable, "-m", f"src.collectors.{collector_name}",
    queries_json, user_id
])
```

**Collectors executed**:
- `collect_twitter_apify`
- `collect_facebook_apify`
- `collect_instagram_apify`
- `collect_tiktok_apify`
- `collect_news_apify`
- `collect_news_from_api`
- `collect_youtube_api`
- `collect_radio_hybrid`
- `collect_rss_nigerian_qatar_indian`
- `collect_rss`
- `collect_social_searcher_api`

Each collector:
1. Imports `run_collectors.py`
2. Calls `run_configurable_collector(target_and_variations, user_id)`
3. Writes CSV to `data/raw/`

---

## üìä Summary Statistics

### Execution Phases
- **5 phases** total
- **3 parallel processing phases** (collection, sentiment, location)
- **2 sequential phases** (data loading, deduplication)

### Database Queries
- **~5-10 queries per cycle** (excluding batch operations)
- **1 bulk insert** for unique records
- **Multiple UPDATE queries** in batches

### External API Calls
- **OpenAI API**: Called by `PresidentialSentimentAnalyzer` and `GovernanceAnalyzer`
- **Collector APIs**: Called by individual collectors (Apify, YouTube, etc.)

### File I/O
- **Read**: Config files, CSV files, log files
- **Write**: CSV files, log files

---

## üîç Notes

1. **Parallel Processing**: 
   - Collection: ThreadPoolExecutor with configurable workers
   - Sentiment: ThreadPoolExecutor with configurable workers
   - Location: ThreadPoolExecutor with configurable workers

2. **Database Sessions**:
   - New session created for each phase
   - Separate sessions for each thread in parallel processing
   - Sessions properly closed after use

3. **Error Handling**:
   - Each phase wrapped in try/except
   - Errors logged but don't stop entire cycle
   - Failed phases reported in logs

4. **Locking**:
   - `_run_task()` uses lock to prevent concurrent cycles
   - Lock stored in `self.task_status`
   - Stuck locks automatically released after timeout

---

## ‚úÖ Verification Checklist

- [x] Entry point identified (`run_cycles.sh`)
- [x] API endpoint identified (`/agent/test-cycle-no-auth`)
- [x] Main method identified (`run_single_cycle_parallel`)
- [x] All 5 phases mapped
- [x] Helper functions identified
- [x] External dependencies listed
- [x] Database operations documented
- [x] File I/O operations documented
- [ ] Line numbers verified (need to check exact line numbers)
- [ ] Subprocess calls verified (need to check exact implementation)

---

## üìù Next Steps

1. Verify exact line numbers for all functions
2. Add call depth/level indicators
3. Document parameter types and return values
4. Add timing information (if available)
5. Cross-reference with UNUSED_CODE_ANALYSIS.md

---

**Last Updated**: 2025-12-27  
**Status**: Initial draft - needs line number verification and refinement













